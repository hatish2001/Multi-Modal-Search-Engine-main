{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Retrival System - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we start by setting up the tools and transformations needed to process images and extract meaningful features from them. First, we import essential libraries like torch for working with deep learning models and torchvision.transforms for applying image transformations. The PIL.Image library helps us handle image files, while faiss is included for efficient similarity search—a crucial part of our image retrieval system. We also bring in os for file handling, numpy for numerical operations, and tqdm to add a handy progress bar during iterative tasks.\n",
    "\n",
    "Next, we define a series of transformations that prepare the images for processing by a deep learning model. These include resizing the image to 256 pixels, applying a center crop to 224 pixels (the input size expected by most pre-trained models), converting the image into a tensor, and normalizing the pixel values using standard mean and standard deviation values. This normalization step is particularly important because it aligns our input images with the statistics of the data the model was originally trained on.\n",
    "\n",
    "Finally, we load a ResNet-50 model pre-trained on ImageNet using torch.hub. This model is designed to extract high-level features from images, making it perfect for our task. By calling model.eval(), we set the model to evaluation mode, ensuring it behaves predictably and doesn't calculate gradients during inference. This setup lays the foundation for extracting image embeddings that we’ll use for retrieval later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/hamza-ubuntu/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/hamza-ubuntu/.conda/envs/image_similarity/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hamza-ubuntu/.conda/envs/image_similarity/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load a pre-trained model (e.g., ResNet-50)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from Images\n",
    "The extract_features function processes an image to extract its feature embeddings using a pre-trained model. It starts by loading the image from the specified path and applying the defined transformations to resize, crop, normalize, and prepare the image for model input. A quick check ensures the image has three channels (RGB), skipping non-standard images with a message for clarity. Using the model in evaluation mode, it computes the feature embeddings, converts them to a NumPy array, and returns them. Errors during processing are handled gracefully, with a helpful message, ensuring the function is robust for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path):\n",
    "    \"\"\"\n",
    "    Extracts features (embeddings) from an image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Extracted features as a numpy array, \n",
    "                    or None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        # Check if the tensor has 3 channels\n",
    "        if img_tensor.shape[1] != 3:\n",
    "            print(f\"Image {image_path} has {img_tensor.shape[1]} channels, skipping.\")\n",
    "            return None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = model(img_tensor).squeeze().numpy()\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Embeddings  \n",
    "\n",
    "This section defines two helper functions, `save_embeddings` and `load_embeddings`, for efficiently managing image embeddings.  \n",
    "\n",
    "- **`save_embeddings`**: This function takes an array of embeddings and saves it to a specified file using Python's `pickle` module. The file is written in binary mode (`wb`), ensuring the data is stored safely and compactly for later use.  \n",
    "\n",
    "- **`load_embeddings`**: This function retrieves the embeddings from a previously saved file. By opening the file in binary read mode (`rb`) and using `pickle.load`, it reconstructs the original embeddings for use in the image retrieval pipeline.  \n",
    "\n",
    "These functions simplify embedding management, allowing for seamless storage and retrieval without the need to recompute features repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_embeddings(embeddings, filename):\n",
    "  \"\"\"\n",
    "  Saves embeddings to a pickle file.\n",
    "\n",
    "  Args:\n",
    "      embeddings (np.ndarray): The embeddings to save.\n",
    "      filename (str): The filename to save the embeddings to.\n",
    "  \"\"\"\n",
    "  with open(filename, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "def load_embeddings(filename):\n",
    "  \"\"\"\n",
    "  Loads embeddings from a pickle file.\n",
    "\n",
    "  Args:\n",
    "      filename (str): The filename to load the embeddings from.\n",
    "\n",
    "  Returns:\n",
    "      np.ndarray: The loaded embeddings.\n",
    "  \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Populating a Faiss Index  \n",
    "\n",
    "This code sets up a **Faiss index**, which is a highly efficient tool for performing similarity searches on high-dimensional data, such as image embeddings. The `add_images_to_index` function is responsible for adding image embeddings to the index, handling both new feature extraction and previously saved embeddings.  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "1. **Faiss Index Initialization**:  \n",
    "   A new Faiss index is created using `faiss.IndexFlatL2`, configured for L2 distance (Euclidean distance) similarity searches. The dimensionality of the index matches the output features of the model’s final layer (`model.fc.out_features`).  \n",
    "\n",
    "2. **Embedding Extraction**:  \n",
    "   If the embeddings file doesn’t already exist, the function loops through all images in the specified folder. For each image, it extracts features using the `extract_features` function. If an image cannot be processed, it’s recorded in `problematic_images`, and the count of corrupted images is tracked. The extracted embeddings are then converted into a NumPy array and saved using the `save_embeddings` function.  \n",
    "\n",
    "3. **Loading Saved Embeddings**:  \n",
    "   If the embeddings file already exists, the function loads the embeddings directly using `load_embeddings`. This avoids recomputing features and speeds up the process.  \n",
    "\n",
    "4. **Adding Embeddings to the Index**:  \n",
    "   Finally, the extracted or loaded embeddings are added to the Faiss index using the `index.add()` method.  \n",
    "\n",
    "### Output  \n",
    "The function returns the populated Faiss index, ready for similarity searches, and a list of problematic images that couldn’t be processed. This ensures both efficiency and robustness when working with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Faiss index\n",
    "index = faiss.IndexFlatL2(model.fc.out_features)\n",
    "\n",
    "def add_images_to_index(image_folder, embeddings_filename=None):\n",
    "    \"\"\"\n",
    "    Adds image embeddings to the Faiss index.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing images.\n",
    "        embeddings_filename (str, optional): Path to the file containing saved embeddings. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        faiss.Index: The Faiss index with added embeddings.\n",
    "    \"\"\"\n",
    "    # Create a Faiss index\n",
    "    index = faiss.IndexFlatL2(model.fc.out_features)\n",
    "\n",
    "    # Extract and save embeddings if the file doesn't exist\n",
    "    if not os.path.exists(embeddings_filename):\n",
    "        embeddings = []\n",
    "        image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
    "        corrupted_count = 0\n",
    "        problematic_images = []\n",
    "        for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "            features = extract_features(image_path)\n",
    "            if features is not None:\n",
    "                embeddings.append(features)\n",
    "            else:\n",
    "                corrupted_count += 1\n",
    "                problematic_images.append(image_path)\n",
    "                print(f\"added {image_path} to problematic_images\")\n",
    "\n",
    "        embeddings = np.array(embeddings)\n",
    "        save_embeddings(embeddings, embeddings_filename)\n",
    "        print(f\"Number of corrupted images: {corrupted_count}\")\n",
    "    else:\n",
    "        # Load embeddings if the file exists\n",
    "        embeddings = load_embeddings(embeddings_filename)\n",
    "\n",
    "    index.add(embeddings)\n",
    "    return index, problematic_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Similar Images  \n",
    "\n",
    "The `search_similar_images` function allows us to find the most similar images in the dataset based on a query image. It leverages the Faiss index, which we previously populated with image embeddings, to perform efficient similarity searches.  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "1. **Query Image Embedding**:  \n",
    "   The function starts by extracting the feature embedding of the query image using the `extract_features` function. This transforms the image into a numerical representation that can be compared against other images in the dataset.  \n",
    "\n",
    "2. **Performing the Search**:  \n",
    "   The query embedding is reshaped and passed to the Faiss index using the `index.search()` method. This method retrieves the top `k` (in this case, 5) most similar images based on their Euclidean distance from the query image. The `distances` array holds the similarity scores, while `indices` contains the indices of the most similar images.  \n",
    "\n",
    "3. **Mapping Indices to Image Paths**:  \n",
    "   The function then maps the indices returned by Faiss back to the corresponding image file paths from the `image_paths` list. This gives us the file paths to the most similar images.  \n",
    "\n",
    "### Output  \n",
    "The function returns a list of file paths to the most similar images. This allows us to efficiently retrieve and display the images that are most closely related to the query, enabling an effective image search experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_images(query_image_path, index, image_paths):\n",
    "    \"\"\"\n",
    "    Searches for similar images based on a query image.\n",
    "\n",
    "    Args:\n",
    "        query_image_path (str): Path to the query image.\n",
    "        index (faiss.Index): The Faiss index containing image embeddings.\n",
    "        image_paths (list): List of image paths.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of paths to the most similar images.\n",
    "    \"\"\"\n",
    "    query_embedding = extract_features(query_image_path)\n",
    "    distances, indices = index.search(query_embedding.reshape(1, -1), k=5)\n",
    "    similar_image_paths = [image_paths[i] for i in indices[0]]\n",
    "    return similar_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Saving Embeddings  \n",
    "\n",
    "This code snippet ensures that the embeddings for all images in the specified folder are extracted and saved for future use. The embeddings are computed and stored in a pickle file to avoid repeated calculations.  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "1. **Setting Paths**:  \n",
    "   The path to the folder containing the images (`image_folder`) is specified, along with the filename (`embeddings_filename`) where the embeddings will be saved. If the embeddings file already exists, it will be loaded directly, skipping the extraction process.  \n",
    "\n",
    "2. **Calling `add_images_to_index`**:  \n",
    "   The function `add_images_to_index` is called to process all images in the folder. If the embeddings file doesn’t exist, it extracts features for each image, handles any problematic images, and saves the embeddings to the specified file. If the embeddings file already exists, it simply loads the embeddings.  \n",
    "\n",
    "3. **Building the Faiss Index**:  \n",
    "   As part of the function, the Faiss index is populated with the image embeddings, making it ready for fast similarity searches. Any problematic images that couldn’t be processed are also returned for further review.  \n",
    "\n",
    "### Output  \n",
    "The code returns the Faiss index, which now contains all the image embeddings, and a list of problematic images that couldn’t be processed. This allows for a smooth image retrieval process and provides useful feedback on any issues with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   1%|          | 140/21384 [00:13<28:08, 12.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/07522-25conta.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/07522-25conta.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   2%|▏         | 466/21384 [01:19<45:37,  7.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23964-2chris01.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23964-2chris01.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   9%|▉         | 1943/21384 [04:29<26:49, 12.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/31303-triumph.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/31303-triumph.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  10%|█         | 2160/21384 [04:54<31:19, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23965-2chris02.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23965-2chris02.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  14%|█▎        | 2917/21384 [06:30<29:18, 10.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23966-2chris03.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23966-2chris03.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  15%|█▌        | 3227/21384 [07:09<26:04, 11.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23961-1james06.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23961-1james06.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  23%|██▎       | 4849/21384 [10:29<26:30, 10.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23967-2chris04.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23967-2chris04.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  24%|██▍       | 5142/21384 [11:07<27:52,  9.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23956-1james01.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23956-1james01.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  25%|██▍       | 5245/21384 [11:20<25:03, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23959-1james04.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23959-1james04.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  25%|██▌       | 5360/21384 [11:35<26:49,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/28351-thebapti.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/28351-thebapti.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  27%|██▋       | 5728/21384 [12:21<23:38, 11.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/26737-lunettes.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/26737-lunettes.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  28%|██▊       | 5955/21384 [12:46<25:21, 10.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/14278-3allegor.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/14278-3allegor.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  33%|███▎      | 7035/21384 [15:05<23:44, 10.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23958-1james03.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23958-1james03.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  34%|███▍      | 7307/21384 [15:40<22:36, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23957-1james02.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23957-1james02.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  36%|███▌      | 7727/21384 [16:34<21:32, 10.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/44237-dutchsqu.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/44237-dutchsqu.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  47%|████▋     | 10026/21384 [21:30<20:23,  9.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23960-1james05.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23960-1james05.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  51%|█████▏    | 10984/21384 [23:31<18:13,  9.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/28344-farnese.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/28344-farnese.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  57%|█████▋    | 12292/21384 [26:17<14:44, 10.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/44328-dance_de.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/44328-dance_de.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  60%|██████    | 12924/21384 [27:37<15:31,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/26670-5outsidf.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/26670-5outsidf.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  62%|██████▏   | 13316/21384 [28:27<13:14, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/38222-2liszt.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/38222-2liszt.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  68%|██████▊   | 14469/21384 [30:54<10:33, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/26735-lu15pha.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/26735-lu15pha.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  71%|███████▏  | 15262/21384 [32:32<09:36, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/26736-lu16abr.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/26736-lu16abr.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  72%|███████▏  | 15374/21384 [32:45<09:22, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/38547-pius_8.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/38547-pius_8.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  78%|███████▊  | 16584/21384 [35:19<06:39, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/39599-3martyr1.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/39599-3martyr1.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  86%|████████▋ | 18466/21384 [39:16<04:35, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/29586-arca2.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/29586-arca2.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  88%|████████▊ | 18821/21384 [39:58<04:11, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/33171-waterloo.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/33171-waterloo.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  90%|████████▉ | 19244/21384 [40:54<03:25, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/38221-1erkel.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/38221-1erkel.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  93%|█████████▎| 19890/21384 [42:20<03:10,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/23968-2chris05.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/23968-2chris05.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  95%|█████████▌| 20383/21384 [43:24<01:37, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image SemArt/Images/44327-circe_ul.jpg: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "added SemArt/Images/44327-circe_ul.jpg to problematic_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 21384/21384 [45:41<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrupted images: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract and save embeddings (if not already done)\n",
    "image_folder = \"SemArt/Images\"\n",
    "embeddings_filename = \"artifacts_image_retrieval/mega_embeddings.pkl\"\n",
    "index, problem_images = add_images_to_index(image_folder, embeddings_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling and Deleting Problematic Images  \n",
    "\n",
    "This section of code focuses on managing problematic images that couldn't be processed during the feature extraction phase. The goal is to save these images' names in a CSV file for tracking and remove them from the system to avoid errors during future operations.  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "1. **Saving Problematic Image Names**:  \n",
    "   The code begins by opening a CSV file (`mega_problematic_images.csv`) in write mode. It writes a header row with the column name \"Image Name\" to clearly label the data. Then, for each problematic image path in `problem_images`, it extracts the file name using `os.path.basename()` and writes it as a new row in the CSV file. This helps maintain a record of all images that were skipped during processing.  \n",
    "\n",
    "2. **Deleting Problematic Images**:  \n",
    "   After saving the names, the code proceeds to delete the problematic images from the system using `os.remove()`. This ensures that any images that caused issues during feature extraction are no longer present in the folder, preventing them from interfering with the rest of the workflow. The deletion of each image is logged with a print statement for tracking purposes.  \n",
    "\n",
    "### Output  \n",
    "The problematic images are removed from the system, and their names are saved in a CSV file for future reference. This improves the overall robustness of the system by ensuring only valid images remain for further processing and retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted problematic image: SemArt/Images/07522-25conta.jpg\n",
      "Deleted problematic image: SemArt/Images/23964-2chris01.jpg\n",
      "Deleted problematic image: SemArt/Images/31303-triumph.jpg\n",
      "Deleted problematic image: SemArt/Images/23965-2chris02.jpg\n",
      "Deleted problematic image: SemArt/Images/23966-2chris03.jpg\n",
      "Deleted problematic image: SemArt/Images/23961-1james06.jpg\n",
      "Deleted problematic image: SemArt/Images/23967-2chris04.jpg\n",
      "Deleted problematic image: SemArt/Images/23956-1james01.jpg\n",
      "Deleted problematic image: SemArt/Images/23959-1james04.jpg\n",
      "Deleted problematic image: SemArt/Images/28351-thebapti.jpg\n",
      "Deleted problematic image: SemArt/Images/26737-lunettes.jpg\n",
      "Deleted problematic image: SemArt/Images/14278-3allegor.jpg\n",
      "Deleted problematic image: SemArt/Images/23958-1james03.jpg\n",
      "Deleted problematic image: SemArt/Images/23957-1james02.jpg\n",
      "Deleted problematic image: SemArt/Images/44237-dutchsqu.jpg\n",
      "Deleted problematic image: SemArt/Images/23960-1james05.jpg\n",
      "Deleted problematic image: SemArt/Images/28344-farnese.jpg\n",
      "Deleted problematic image: SemArt/Images/44328-dance_de.jpg\n",
      "Deleted problematic image: SemArt/Images/26670-5outsidf.jpg\n",
      "Deleted problematic image: SemArt/Images/38222-2liszt.jpg\n",
      "Deleted problematic image: SemArt/Images/26735-lu15pha.jpg\n",
      "Deleted problematic image: SemArt/Images/26736-lu16abr.jpg\n",
      "Deleted problematic image: SemArt/Images/38547-pius_8.jpg\n",
      "Deleted problematic image: SemArt/Images/39599-3martyr1.jpg\n",
      "Deleted problematic image: SemArt/Images/29586-arca2.jpg\n",
      "Deleted problematic image: SemArt/Images/33171-waterloo.jpg\n",
      "Deleted problematic image: SemArt/Images/38221-1erkel.jpg\n",
      "Deleted problematic image: SemArt/Images/23968-2chris05.jpg\n",
      "Deleted problematic image: SemArt/Images/44327-circe_ul.jpg\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('artifacts_image_retrieval/mega_problematic_images.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Image Name'])\n",
    "    for image_path in problem_images:\n",
    "        filename = os.path.basename(image_path)\n",
    "        writer.writerow([filename])\n",
    "\n",
    "for image_path in problem_images:\n",
    "    os.remove(image_path)\n",
    "    print(f\"Deleted problematic image: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SemArt/Images/30580-stil_lif.jpg', 'SemArt/Images/08686-10vase.jpg', 'SemArt/Images/34434-stillif.jpg', 'SemArt/Images/26087-24nojov4.jpg', 'SemArt/Images/37724-stillife.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings and create index (if embeddings are already saved)\n",
    "# index = add_images_to_index(image_folder, embeddings_filename)\n",
    "\n",
    "# Search for similar images\n",
    "query_image_path = \"test_set/vase1.jpeg\"\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
    "similar_images = search_similar_images(query_image_path, index, image_paths)\n",
    "print(similar_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Description  \n",
    "\n",
    "## Creating the Image Paths CSV  \n",
    "\n",
    "This section of code is used to generate a CSV file containing the file paths of all images in the dataset. This CSV file acts as a reference to map image paths to their corresponding embeddings, which is essential for performing image retrieval and matching operations later on.  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "1. **Opening the CSV File**:  \n",
    "   The code opens a CSV file (`image_paths.csv`) in write mode. The `csv.writer()` function is used to write the image paths into the file, ensuring each image path is stored in its own row.  \n",
    "\n",
    "2. **Writing Image Paths**:  \n",
    "   The loop iterates over the list of image paths (`image_paths`) and writes each one into the CSV file. Each path is written as a single value in a row. This CSV file can later be loaded to easily retrieve the image paths when matching them with the corresponding embeddings in the Faiss index.  \n",
    "\n",
    "### Output  \n",
    "The code generates a CSV file (`image_paths.csv`) that contains a list of all image paths. This file is crucial for associating image paths with their corresponding embeddings, enabling efficient image search and retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED TO CREATE THE IMAGE PATH CSV THAT YOU CAN USE LATER ON TO FETCH IMAGE NAMES TO CORRESPONDING EMBEDDINGS\n",
    "#  with open('artifacts_image_retrieval/image_paths.csv', 'w', newline='') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for image_path in image_paths:\n",
    "#         writer.writerow([image_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Image Paths and Searching for Similar Images  \n",
    "\n",
    "In this section, the code performs two tasks: loading the list of image paths from a CSV file and using that list to find similar images based on a query image.  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "1. **Loading Image Paths**:  \n",
    "   The code begins by opening the CSV file (`image_paths.csv`) in read mode. It uses the `csv.reader()` function to read each row and extracts the image paths from the first column. These paths are then stored in the `image_paths_loaded` list. This list serves as a reference for all images in the dataset that will be compared against the query image.  \n",
    "\n",
    "2. **Searching for Similar Images**:  \n",
    "   The query image path (`query_image_path`) is specified, pointing to a specific image file (in this case, `\"test_set/vase1.jpeg\"`). The function `search_similar_images` is called, which takes the query image, the Faiss index, and the loaded image paths as inputs. It then retrieves the top 5 most similar images from the dataset based on the query image’s feature embedding.  \n",
    "\n",
    "3. **Displaying Results**:  \n",
    "   The list of similar image paths (`similar_images`) is printed to the console. This provides the user with the file paths of the images that are most similar to the query image.  \n",
    "\n",
    "### Output  \n",
    "The output is a list of file paths to the most similar images in the dataset, allowing the user to visually compare the query image with others that are closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SemArt/Images/30580-stil_lif.jpg', 'SemArt/Images/08686-10vase.jpg', 'SemArt/Images/34434-stillif.jpg', 'SemArt/Images/26087-24nojov4.jpg', 'SemArt/Images/37724-stillife.jpg']\n"
     ]
    }
   ],
   "source": [
    "with open('artifacts_image_retrieval/image_paths.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    image_paths_loaded = [row[0] for row in reader]\n",
    "query_image_path = \"test_set/vase1.jpeg\"\n",
    "similar_images = search_similar_images(query_image_path, index, image_paths_loaded)\n",
    "print(similar_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_similarity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
